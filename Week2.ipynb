{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b82ac2e",
   "metadata": {},
   "source": [
    "Week 2: Generative Model (VAE) â€” Train & Validate\n",
    "\n",
    "Load Week 1 artifacts, train a simple VAE on 20-trading-day forward return windows across 10 stocks, validate realism (KS tests, moments, autocorr/covariance), and save 5,000 synthetic 4-week scenarios for Week 3 (clustering & regime states)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a3a6f97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Week1 from: C:\\Users\\Krish\\OneDrive - Curtin\\Desktop\\University\\University\\2025 Sem 2\\MATH3004\\22155584_Week_2\n",
      "Saving Week2 outputs to: C:\\Users\\Krish\\OneDrive - Curtin\\Desktop\\University\\University\\2025 Sem 2\\MATH3004\\22155584_Week_2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Cell 1: Imports & config ---\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import ks_2samp\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# -------- Paths (Week1 files are in Week2 folder) --------\n",
    "WEEK2_DIR = Path(__file__).resolve().parent if \"__file__\" in globals() else Path(\".\").resolve()\n",
    "WEEK1_DIR = WEEK2_DIR                               # since you put Week 1 outputs here\n",
    "OUT_DIR   = WEEK2_DIR                               # save Week 2 outputs here\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Week 1 artifacts\n",
    "FEAT_FP = WEEK1_DIR / \"features_week1.parquet\"\n",
    "RET_FP  = WEEK1_DIR / \"returns_week1.parquet\"\n",
    "\n",
    "for f in [FEAT_FP, RET_FP]:\n",
    "    if not f.exists():\n",
    "        raise FileNotFoundError(f\"Missing file: {f}\")\n",
    "\n",
    "print(\"Loading Week1 from:\", WEEK1_DIR)\n",
    "print(\"Saving Week2 outputs to:\", OUT_DIR)\n",
    "\n",
    "# -------- Universe / horizon / training hyperparams --------\n",
    "TICKERS     = [\"AAPL\",\"MSFT\",\"AMZN\",\"GOOGL\",\"META\",\"NVDA\",\"JPM\",\"JNJ\",\"XOM\",\"WMT\"]\n",
    "H           = 20      # horizon: 20 trading days (~4 weeks)\n",
    "BATCH_SIZE  = 256\n",
    "EPOCHS      = 60\n",
    "LR          = 1e-3\n",
    "LATENT_DIM  = 16\n",
    "HIDDEN_DIM  = 128\n",
    "SEED        = 42\n",
    "N_SAMPLES   = 5000    # scenarios to generate for Week 3\n",
    "\n",
    "# Reproducibility\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab233e82",
   "metadata": {},
   "source": [
    "Load Week 1 returns & build forward 20-day windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e190b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training windows: (2434, 20, 10) | anchors: 2434\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 2: Load Week 1 returns and build forward windows ---\n",
    "returns = pd.read_parquet(RET_FP)\n",
    "\n",
    "# returns has MultiIndex columns: (ticker, field='ret')\n",
    "cols = [(t, \"ret\") for t in TICKERS]\n",
    "returns = returns.loc[:, cols].copy()\n",
    "returns.columns = pd.MultiIndex.from_product([TICKERS, [\"ret\"]], names=[\"ticker\",\"field\"])\n",
    "\n",
    "R = returns.droplevel(\"field\", axis=1).dropna(how=\"any\")  # [T, N_assets]\n",
    "\n",
    "# Build strictly forward windows: for each anchor t, sample is returns[t+1 : t+H]\n",
    "X = []\n",
    "anchors = []\n",
    "T_len = len(R)\n",
    "for i in range(T_len - H):\n",
    "    win = R.iloc[i+1:i+1+H].to_numpy()\n",
    "    if np.isfinite(win).all():\n",
    "        X.append(win)\n",
    "        anchors.append(R.index[i])\n",
    "\n",
    "X = np.asarray(X)  # [num_samples, H, N_assets]\n",
    "print(\"Training windows:\", X.shape, \"| anchors:\", len(anchors))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e354c007",
   "metadata": {},
   "source": [
    "Train/val split & scaling (fit on train only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4889676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Val (z-scored): (1947, 200) (487, 200)\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 3: Split + scale ---\n",
    "num_samples, H_win, N_assets = X.shape\n",
    "D = H_win * N_assets\n",
    "\n",
    "X_flat = X.reshape(num_samples, D)\n",
    "\n",
    "X_train, X_val = train_test_split(X_flat, test_size=0.2, random_state=SEED, shuffle=True)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_z = scaler.fit_transform(X_train)\n",
    "X_val_z   = scaler.transform(X_val)\n",
    "\n",
    "print(\"Train/Val (z-scored):\", X_train_z.shape, X_val_z.shape)\n",
    "\n",
    "# Save scaler meta\n",
    "scaler_meta = {\n",
    "    \"mean\": scaler.mean_.tolist(),\n",
    "    \"scale\": scaler.scale_.tolist(),\n",
    "    \"H\": H, \"N_assets\": N_assets, \"tickers\": TICKERS\n",
    "}\n",
    "with open(OUT_DIR/\"vae_scaler.json\",\"w\") as f:\n",
    "    json.dump(scaler_meta, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e1fb0f",
   "metadata": {},
   "source": [
    "Dataset/DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "431a43ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 4: Dataset / DataLoader ---\n",
    "class WindowDataset(Dataset):\n",
    "    def __init__(self, Xz):\n",
    "        self.Xz = torch.tensor(Xz, dtype=torch.float32)\n",
    "    def __len__(self):\n",
    "        return self.Xz.shape[0]\n",
    "    def __getitem__(self, idx):\n",
    "        return self.Xz[idx]\n",
    "\n",
    "train_ds = WindowDataset(X_train_z)\n",
    "val_ds   = WindowDataset(X_val_z)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=False)\n",
    "val_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ebf629",
   "metadata": {},
   "source": [
    "VAE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b21f34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 5: VAE model ---\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=128, latent_dim=16):\n",
    "        super().__init__()\n",
    "        self.enc = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.mu = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.logvar = nn.Linear(hidden_dim, latent_dim)\n",
    "\n",
    "        self.dec = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, input_dim),\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.enc(x)\n",
    "        return self.mu(h), self.logvar(h)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.dec(z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        recon = self.decode(z)\n",
    "        return recon, mu, logvar\n",
    "\n",
    "def vae_loss(recon, x, mu, logvar):\n",
    "    recon_loss = nn.functional.mse_loss(recon, x, reduction='mean')\n",
    "    kl = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return recon_loss + kl, recon_loss, kl\n",
    "\n",
    "vae = VAE(input_dim=D, hidden_dim=HIDDEN_DIM, latent_dim=LATENT_DIM).to(device)\n",
    "opt = torch.optim.Adam(vae.parameters(), lr=LR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d88ccf7",
   "metadata": {},
   "source": [
    "Train with early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f35bafd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | train 1.0119 | val 1.0171\n",
      "Epoch 002 | train 1.0041 | val 1.0143\n",
      "Epoch 003 | train 1.0025 | val 1.0134\n",
      "Epoch 004 | train 1.0019 | val 1.0128\n",
      "Epoch 005 | train 1.0014 | val 1.0124\n",
      "Epoch 006 | train 1.0013 | val 1.0124\n",
      "Epoch 007 | train 1.0009 | val 1.0121\n",
      "Epoch 008 | train 1.0010 | val 1.0123\n",
      "Epoch 009 | train 1.0007 | val 1.0122\n",
      "Epoch 010 | train 1.0007 | val 1.0120\n",
      "Epoch 011 | train 1.0006 | val 1.0121\n",
      "Epoch 012 | train 1.0006 | val 1.0119\n",
      "Epoch 013 | train 1.0005 | val 1.0121\n",
      "Epoch 014 | train 1.0004 | val 1.0118\n",
      "Epoch 015 | train 1.0004 | val 1.0118\n",
      "Epoch 016 | train 1.0004 | val 1.0118\n",
      "Epoch 017 | train 1.0003 | val 1.0117\n",
      "Epoch 018 | train 1.0003 | val 1.0114\n",
      "Epoch 019 | train 1.0003 | val 1.0115\n",
      "Epoch 020 | train 1.0003 | val 1.0117\n",
      "Epoch 021 | train 1.0003 | val 1.0115\n",
      "Epoch 022 | train 1.0002 | val 1.0116\n",
      "Epoch 023 | train 1.0003 | val 1.0115\n",
      "Epoch 024 | train 1.0003 | val 1.0117\n",
      "Epoch 025 | train 1.0002 | val 1.0115\n",
      "Epoch 026 | train 1.0002 | val 1.0116\n",
      "Early stopping.\n",
      "Best val: 1.011382775629815\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 6: Train ---\n",
    "best_val = float(\"inf\")\n",
    "patience, patience_left = 8, 8\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    vae.train()\n",
    "    tr_loss = 0.0\n",
    "    for xb in train_loader:\n",
    "        xb = xb.to(device)\n",
    "        recon, mu, logvar = vae(xb)\n",
    "        loss, rl, kl = vae_loss(recon, xb, mu, logvar)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        tr_loss += loss.item() * xb.size(0)\n",
    "    tr_loss /= len(train_ds)\n",
    "\n",
    "    vae.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0.0\n",
    "        for xb in val_loader:\n",
    "            xb = xb.to(device)\n",
    "            recon, mu, logvar = vae(xb)\n",
    "            loss, rl, kl = vae_loss(recon, xb, mu, logvar)\n",
    "            val_loss += loss.item() * xb.size(0)\n",
    "        val_loss /= len(val_ds)\n",
    "\n",
    "    print(f\"Epoch {epoch:03d} | train {tr_loss:.4f} | val {val_loss:.4f}\")\n",
    "\n",
    "    if val_loss + 1e-6 < best_val:\n",
    "        best_val = val_loss\n",
    "        patience_left = patience\n",
    "        torch.save(vae.state_dict(), OUT_DIR / \"vae_model.pt\")\n",
    "    else:\n",
    "        patience_left -= 1\n",
    "        if patience_left == 0:\n",
    "            print(\"Early stopping.\")\n",
    "            break\n",
    "\n",
    "vae.load_state_dict(torch.load(OUT_DIR / \"vae_model.pt\", map_location=device))\n",
    "vae.eval()\n",
    "print(\"Best val:\", best_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880658d9",
   "metadata": {},
   "source": [
    "Generate 5,000 synthetic 20-day scenarios & save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08632898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: synthetic_paths_20d.parquet\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 7: Generate synthetic scenarios ---\n",
    "with torch.no_grad():\n",
    "    z = torch.randn(N_SAMPLES, LATENT_DIM, device=device)\n",
    "    gen_z = vae.decode(z).cpu().numpy()  # [N_SAMPLES, D]\n",
    "\n",
    "# Inverse scale to original return space\n",
    "gen_flat = scaler.inverse_transform(gen_z)                 # [N_SAMPLES, D]\n",
    "gen_seq  = gen_flat.reshape(N_SAMPLES, H, len(TICKERS))    # [N, H, N_assets]\n",
    "\n",
    "# Tidy DataFrame (index = scenario_id, day; columns = tickers)\n",
    "mux = pd.MultiIndex.from_product([range(N_SAMPLES), range(1, H+1)],\n",
    "                                 names=[\"scenario_id\",\"day\"])\n",
    "gen_df = pd.DataFrame(gen_seq.reshape(N_SAMPLES*H, len(TICKERS)),\n",
    "                      index=mux, columns=TICKERS)\n",
    "\n",
    "# Save artifacts for Week 3\n",
    "gen_df.to_parquet(OUT_DIR / \"synthetic_paths_20d.parquet\")\n",
    "np.save(OUT_DIR / \"synthetic_paths_20d.npy\", gen_seq)\n",
    "print(\"Saved:\", (OUT_DIR / \"synthetic_paths_20d.parquet\").name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ecd7bf2",
   "metadata": {},
   "source": [
    "Validation: KS tests, moments, autocorr, covariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fbbe0020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation report: vae_validation_report.json\n",
      "{\n",
      "  \"ks_per_asset\": {\n",
      "    \"AAPL\": {\n",
      "      \"statistic\": 0.46813014790468355,\n",
      "      \"pvalue\": 0.0\n",
      "    },\n",
      "    \"MSFT\": {\n",
      "      \"statistic\": 0.46683492193919474,\n",
      "      \"pvalue\": 0.0\n",
      "    },\n",
      "    \"AMZN\": {\n",
      "      \"statistic\": 0.4633198438783894,\n",
      "      \"pvalue\": 0.0\n",
      "    },\n",
      "    \"GOOGL\": {\n",
      "      \"statistic\": 0.48521511092851277,\n",
      "      \"pvalue\": 0.0\n",
      "    },\n",
      "    \"META\": {\n",
      "      \"statistic\": 0.46494448644207065,\n",
      "      \"pvalue\": 0.0\n",
      "    },\n",
      "    \"NVDA\": {\n",
      "      \"statistic\": 0.4674969597370583,\n",
      "      \"pvalue\": 0.0\n",
      "    },\n",
      "    \"JPM\": {\n",
      "      \"statistic\": 0.4752853820870994,\n",
      "      \"pvalue\": 0.0\n",
      "    },\n",
      "    \"JNJ\": {\n",
      "      \"statistic\": 0.47324169268693506,\n",
      "      \"pvalue\": 0.0\n",
      "    },\n",
      "    \"XOM\": {\n",
      "      \"statistic\": 0.47078898110106826,\n",
      "      \"pvalue\": 0.0\n",
      "    },\n",
      "    \"WMT\": {\n",
      "      \"statistic\": 0.4711023418241578,\n",
      "      \"pvalue\": 0.0\n",
      "    }\n",
      "  },\n",
      "  \"moments\": {\n",
      "    \"AAPL\": {\n",
      "      \"real_mean\": 0.0010208849910188264,\n",
      "      \"gen_mean\": 0.0010159678058698773,\n",
      "      \"real_std\": 0.01844183787167263,\n",
      "      \"gen_std\": 0.0003184584784321487\n",
      "    },\n",
      "    \"MSFT\": {\n",
      "      \"real_mean\": 0.0011228375496063357,\n",
      "      \"gen_mean\": 0.001062024850398302,\n",
      "      \"real_std\": 0.017011119534829942,\n",
      "      \"gen_std\": 0.0002854377671610564  ...\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 8: Validation metrics ---\n",
    "report = {}\n",
    "\n",
    "# Flatten across time for marginal checks\n",
    "real_all = X.reshape(X.shape[0]*H, len(TICKERS))\n",
    "gen_all  = gen_seq.reshape(gen_seq.shape[0]*H, len(TICKERS))\n",
    "\n",
    "# 8.1 KS per asset\n",
    "ks_results = {}\n",
    "for i, t in enumerate(TICKERS):\n",
    "    ks = ks_2samp(real_all[:, i], gen_all[:, i])\n",
    "    ks_results[t] = {\"statistic\": float(ks.statistic), \"pvalue\": float(ks.pvalue)}\n",
    "report[\"ks_per_asset\"] = ks_results\n",
    "\n",
    "# 8.2 Moments\n",
    "moments = {}\n",
    "for i, t in enumerate(TICKERS):\n",
    "    moments[t] = {\n",
    "        \"real_mean\": float(real_all[:, i].mean()),\n",
    "        \"gen_mean\":  float(gen_all[:, i].mean()),\n",
    "        \"real_std\":  float(real_all[:, i].std()),\n",
    "        \"gen_std\":   float(gen_all[:, i].std()),\n",
    "    }\n",
    "report[\"moments\"] = moments\n",
    "\n",
    "# 8.3 Mean lag-1 autocorr across sequences (per asset)\n",
    "def mean_lag1_autocorr(seq):  # seq: [num_samples, H, N]\n",
    "    ac = []\n",
    "    for j in range(seq.shape[2]):\n",
    "        x = seq[:, :, j]\n",
    "        x0 = x[:, :-1] - x[:, :-1].mean(axis=1, keepdims=True)\n",
    "        x1 = x[:,  1:] - x[:, :-1].mean(axis=1, keepdims=True)\n",
    "        num = (x1 * x0).mean()\n",
    "        den = (x0**2).mean()\n",
    "        ac.append(float(num/den) if den != 0 else 0.0)\n",
    "    return ac\n",
    "\n",
    "ac_real = mean_lag1_autocorr(X)\n",
    "ac_gen  = mean_lag1_autocorr(gen_seq)\n",
    "report[\"lag1_autocorr\"] = {t: {\"real\": ac_real[i], \"gen\": ac_gen[i]} for i, t in enumerate(TICKERS)}\n",
    "\n",
    "# 8.4 Cross-asset covariance diff (Frobenius norm)\n",
    "cov_real = np.cov(real_all.T)\n",
    "cov_gen  = np.cov(gen_all.T)\n",
    "cov_diff = np.linalg.norm(cov_real - cov_gen, ord=\"fro\")\n",
    "report[\"covariance_frobenius_diff\"] = float(cov_diff)\n",
    "\n",
    "with open(OUT_DIR/\"vae_validation_report.json\",\"w\") as f:\n",
    "    json.dump(report, f, indent=2)\n",
    "\n",
    "print(\"Validation report:\", (OUT_DIR/\"vae_validation_report.json\").name)\n",
    "print(json.dumps(report, indent=2)[:1200], \" ...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1c503f",
   "metadata": {},
   "source": [
    "Hand-off summary (for Week 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ab3aed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artifacts ready for Week 3:\n",
      " â€¢ vae_model.pt\n",
      " â€¢ vae_scaler.json\n",
      " â€¢ synthetic_paths_20d.parquet\n",
      " â€¢ synthetic_paths_20d.npy\n",
      " â€¢ vae_validation_report.json\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 9: Hand-off summary ---\n",
    "print(\"Artifacts ready for Week 3:\")\n",
    "for p in [\n",
    "    OUT_DIR / \"vae_model.pt\",\n",
    "    OUT_DIR / \"vae_scaler.json\",\n",
    "    OUT_DIR / \"synthetic_paths_20d.parquet\",\n",
    "    OUT_DIR / \"synthetic_paths_20d.npy\",\n",
    "    OUT_DIR / \"vae_validation_report.json\",\n",
    "]:\n",
    "    print(\" â€¢\", p.name)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
